{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuVtvIGStlby"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers datasets gdown\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Config,\n",
        ")"
      ],
      "metadata": {
        "id": "5NKwzl0-uIJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown \"https://drive.google.com/uc?id=1e0vl0UwBiWyQgwWevul_sDhwtkb7ASWP\"\n",
        "!gdown \"https://drive.google.com/uc?id=13YfUdSJDPvkn4_weszCkKIIHjnkpEQ5T\"\n",
        "!gdown \"https://drive.google.com/uc?id=1AFsyj4RepOzCPWQ7CIBQoqOEqRi4KFEO\""
      ],
      "metadata": {
        "id": "XSUObY02wYCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = pandas.read_csv(\"t5-train-laeme-data.csv\")\n",
        "data_valid = pandas.read_csv(\"t5-valid-laeme-data.csv\")\n",
        "data_test = pandas.read_csv(\"t5-test-laeme-data.csv\")\n",
        "\n",
        "dataset = DatasetDict()\n",
        "\n",
        "dataset[\"test\"] = Dataset.from_pandas(data_test)\n",
        "dataset[\"train\"] = Dataset.from_pandas(data_train)\n",
        "dataset[\"valid\"] = Dataset.from_pandas(data_valid)"
      ],
      "metadata": {
        "id": "v1dN3y7vwdvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dataset):\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        input = tokenizer(\n",
        "            dataset[\"input\"],\n",
        "            padding=True,\n",
        "            add_special_tokens=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        target = tokenizer(\n",
        "            dataset[\"target\"],\n",
        "            padding=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input[\"labels\"] = target[\"input_ids\"].masked_fill(\n",
        "            target.attention_mask.ne(1), -100\n",
        "        )\n",
        "        return input"
      ],
      "metadata": {
        "id": "4Mpm1i6n60uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHECKPOINT = \"google/byt5-small\""
      ],
      "metadata": {
        "id": "AmIdP5ls_Lka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)"
      ],
      "metadata": {
        "id": "6K1xQtkq_KMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(\n",
        "    preprocess, batched=True, remove_columns=[\"input\", \"prefix\", \"id\", \"target\"]\n",
        ")"
      ],
      "metadata": {
        "id": "erEBkJNn868g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Here is how to decode the text with ByT5 tokenizer === #\n",
        "# Mind you, the text is padded\n",
        "\n",
        "# decoded_text = tokenizer.decode(tokenized_dataset[\"test\"][\"input_ids\"][1], skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "FkK2QFKOnkQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(CHECKPOINT)\n",
        "\n",
        "config = T5Config.from_pretrained(CHECKPOINT)\n",
        "config.num_decoder_layers = 2\n",
        "config.num_layers = 6\n",
        "config.d_kv = 64\n",
        "config.d_model = 256\n",
        "config.d_ff = 512\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    generation_num_beams=5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=32,  # 256\n",
        "    per_device_eval_batch_size=64,  # 512\n",
        "    num_train_epochs=5,  # 10\n",
        "    gradient_accumulation_steps=1,  # 2\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=1000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    fp16=True, \n",
        "    output_dir=\"contents\",\n",
        "    logging_steps=1000,\n",
        "    save_steps=20000,  # 5000\n",
        "    eval_steps=20000,  # 5000\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "D910UPyAy9WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"valid\"],\n",
        "    compute_metrics=None,\n",
        ")"
      ],
      "metadata": {
        "id": "BxfTExb2nTmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "yo_KKcPK1izt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}